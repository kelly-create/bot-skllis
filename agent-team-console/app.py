#!/usr/bin/env python3
import json
import os
import re
import sqlite3
import subprocess
import threading
import time
import shutil
from contextlib import contextmanager
from datetime import datetime
from functools import wraps

from flask import Flask, abort, flash, g, jsonify, redirect, render_template, request, send_from_directory, session, url_for
from werkzeug.utils import secure_filename

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.getenv("ATC_DB_PATH", os.path.join(BASE_DIR, "data", "tasks.db"))
ADMIN_USERNAME = os.getenv("ATC_ADMIN_USERNAME", "root")
ADMIN_PASSWORD = os.getenv("ATC_ADMIN_PASSWORD", "k5348988")
APP_SECRET = os.getenv("ATC_APP_SECRET", "change-me-now")
WORKDIR = os.getenv("ATC_WORKDIR", BASE_DIR)
DEFAULT_MAX_CONCURRENT = int(os.getenv("ATC_MAX_CONCURRENT", "4"))
ARTIFACT_ROOT = os.getenv("ATC_ARTIFACT_ROOT", os.path.join(BASE_DIR, "artifacts"))

os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
os.makedirs(ARTIFACT_ROOT, exist_ok=True)

app = Flask(__name__)
app.secret_key = APP_SECRET
app.config["MAX_CONTENT_LENGTH"] = 200 * 1024 * 1024  # 200MB

running_processes = {}


class ConcurrencyLimiter:
    def __init__(self, limit: int):
        self._limit = max(1, int(limit))
        self._running = 0
        self._lock = threading.Lock()
        self._cond = threading.Condition(self._lock)

    @contextmanager
    def acquire(self):
        with self._cond:
            while self._running >= self._limit:
                self._cond.wait(timeout=1)
            self._running += 1
        try:
            yield
        finally:
            with self._cond:
                self._running -= 1
                self._cond.notify_all()

    def set_limit(self, limit: int):
        with self._cond:
            self._limit = max(1, int(limit))
            self._cond.notify_all()

    def get_limit(self) -> int:
        with self._lock:
            return self._limit

    def get_running(self) -> int:
        with self._lock:
            return self._running


limiter = ConcurrencyLimiter(DEFAULT_MAX_CONCURRENT)


def now_str():
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")


def format_size(num_bytes: int) -> str:
    size = float(num_bytes)
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size < 1024 or unit == "TB":
            return f"{size:.1f}{unit}" if unit != "B" else f"{int(size)}B"
        size /= 1024


def list_artifacts(max_items: int = 300):
    out = []
    for root, _, files in os.walk(ARTIFACT_ROOT):
        for name in files:
            full = os.path.join(root, name)
            rel = os.path.relpath(full, ARTIFACT_ROOT)
            try:
                st = os.stat(full)
            except FileNotFoundError:
                continue
            out.append(
                {
                    "name": name,
                    "rel_path": rel,
                    "size": st.st_size,
                    "size_human": format_size(st.st_size),
                    "mtime": datetime.utcfromtimestamp(st.st_mtime).strftime("%Y-%m-%d %H:%M:%S UTC"),
                    "ts": st.st_mtime,
                }
            )
    out.sort(key=lambda x: x["ts"], reverse=True)
    return out[:max_items]


def task_artifact_dirs(task_id: int):
    base = os.path.join(ARTIFACT_ROOT, f"task_{task_id}")
    in_dir = os.path.join(base, "input")
    out_dir = os.path.join(base, "output")
    os.makedirs(in_dir, exist_ok=True)
    os.makedirs(out_dir, exist_ok=True)
    return base, in_dir, out_dir


def list_task_files(task_id: int, kind: str):
    base, in_dir, out_dir = task_artifact_dirs(task_id)
    target = in_dir if kind == "input" else out_dir
    out = []
    if not os.path.isdir(target):
        return out
    for name in os.listdir(target):
        full = os.path.join(target, name)
        if not os.path.isfile(full):
            continue
        st = os.stat(full)
        rel = os.path.relpath(full, base)
        out.append(
            {
                "name": name,
                "rel_path": rel,
                "size_human": format_size(st.st_size),
                "mtime": datetime.utcfromtimestamp(st.st_mtime).strftime("%Y-%m-%d %H:%M:%S UTC"),
                "ts": st.st_mtime,
            }
        )
    out.sort(key=lambda x: x["ts"], reverse=True)
    return out


def infer_business_phase(task) -> str:
    status = (task["status"] or "").strip().lower()
    if status == "pending":
        return "å¾…ç†è§£"
    if status == "running":
        return "æ‰§è¡Œä¸­"
    if status == "done":
        return "å¾…ä½ ç¡®è®¤"
    if status == "failed":
        return "éœ€è¿”å·¥"
    return "å…¶ä»–"


def classify_output_files(output_files):
    packs, reports, audits, others = [], [], [], []
    for f in output_files:
        name = (f.get("name") or "").lower()
        is_pack = name.endswith(".zip") or name.endswith(".7z") or name.endswith(".tar.gz") or name.endswith(".tgz")
        is_audit = ("audit" in name) or ("å®¡è®¡" in name) or name.endswith(".log")
        is_report = name.endswith(".md") or name.endswith(".json") or name.endswith(".txt") or name.endswith(".csv") or name.endswith(".pdf")

        if is_pack:
            packs.append(f)
        elif is_audit:
            audits.append(f)
        elif is_report:
            reports.append(f)
        else:
            others.append(f)
    return {"packs": packs, "reports": reports, "audits": audits, "others": others}


def build_delivery_overview(task, output_files, logs):
    status = (task["status"] or "").strip().lower()
    rc = task["return_code"]
    if status == "done" and (rc in (0, "0", None) or rc == 0):
        headline = "âœ… ä»»åŠ¡å·²å®Œæˆï¼Œå¯ç›´æ¥æŸ¥çœ‹å¹¶ä¸‹è½½äº¤ä»˜ç‰©"
        next_action = "ä¼˜å…ˆä¸‹è½½â€œäº¤ä»˜å‹ç¼©åŒ…â€ï¼Œç¡®è®¤ç»“æœåå¯å½’æ¡£ä»»åŠ¡ã€‚"
        progress = 100
    elif status == "running":
        headline = "â³ ä»»åŠ¡æ‰§è¡Œä¸­ï¼Œæ­£åœ¨æŒç»­äº§å‡º"
        next_action = "å¯å…ˆæŸ¥çœ‹å®æ—¶æ—¥å¿—ï¼Œç­‰å¾…è¿›å…¥â€œå¾…ä½ ç¡®è®¤â€ã€‚"
        progress = 60
    elif status == "failed":
        headline = "âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œéœ€è¦è¿”å·¥"
        next_action = "æŸ¥çœ‹å¤±è´¥æ—¥å¿—å¹¶é‡ç½®ä»»åŠ¡ï¼Œå¿…è¦æ—¶è¡¥å……é™„ä»¶æˆ–è¯´æ˜ã€‚"
        progress = 100
    else:
        headline = "ğŸ“ ä»»åŠ¡å¾…æ‰§è¡Œ"
        next_action = "ç¡®è®¤ä»»åŠ¡æè¿°ä¸é™„ä»¶åï¼Œç‚¹å‡»â€œå¯åŠ¨â€ã€‚"
        progress = 12

    latest_line = ""
    for row in reversed(logs):
        line = (row["line"] or "").strip()
        if not line:
            continue
        latest_line = line
        if not line.startswith("[SYSTEM]"):
            break

    groups = classify_output_files(output_files)
    primary_pack = groups["packs"][0] if groups["packs"] else None

    return {
        "headline": headline,
        "next_action": next_action,
        "progress": progress,
        "latest_line": latest_line,
        "groups": groups,
        "primary_pack": primary_pack,
    }


def safe_join_under(root: str, rel_path: str):
    safe_full = os.path.realpath(os.path.join(root, rel_path))
    root_real = os.path.realpath(root)
    if not safe_full.startswith(root_real + os.sep) and safe_full != root_real:
        return None
    return safe_full


@contextmanager
def db_conn():
    conn = sqlite3.connect(DB_PATH, timeout=30)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
        conn.commit()
    finally:
        conn.close()


def init_db():
    with db_conn() as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                description TEXT,
                task_type TEXT,
                assignee TEXT,
                priority TEXT DEFAULT 'P2',
                status TEXT DEFAULT 'pending',
                command TEXT,
                created_at TEXT,
                updated_at TEXT,
                started_at TEXT,
                finished_at TEXT,
                return_code INTEGER
            )
            """
        )
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS task_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                ts TEXT,
                line TEXT,
                FOREIGN KEY(task_id) REFERENCES tasks(id)
            )
            """
        )
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS app_settings (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at TEXT
            )
            """
        )
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS roles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                code TEXT UNIQUE NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                default_model TEXT,
                enabled INTEGER DEFAULT 1,
                created_at TEXT,
                updated_at TEXT
            )
            """
        )
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS workflows (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                code TEXT UNIQUE NOT NULL,
                name TEXT NOT NULL,
                description TEXT,
                stages_json TEXT,
                default_task_type TEXT,
                default_assignee TEXT,
                command_template TEXT,
                enabled INTEGER DEFAULT 1,
                created_at TEXT,
                updated_at TEXT
            )
            """
        )

        # é»˜è®¤å…¨å±€è§’è‰²ï¼ˆåˆ›å»ºä¸€æ¬¡ï¼Œåç»­å¯åœ¨é¡µé¢ç»´æŠ¤ï¼‰
        default_roles = [
            ("Lead Agent", "Lead Agent", "ä»»åŠ¡æ€»æ§ä¸ç¼–æ’", "sydney-proxy/claude-opus-4-6-thinking"),
            ("@developer", "å¼€å‘ Agent", "å®ç°åŠŸèƒ½ã€æ”¹ä»£ç ã€ä¿®å¤é—®é¢˜", "sydney-proxy/gpt-5.3-codex"),
            ("@tester", "æµ‹è¯• Agent", "å›å½’æµ‹è¯•ã€è¾¹ç•ŒéªŒè¯ã€å¤ç°é—®é¢˜", "sydney-proxy/gpt-5.2-codex"),
            ("@verifier", "éªŒè¯ Agent", "æŒ‰éªŒæ”¶æ ‡å‡†åšæœ€ç»ˆæ ¸å¯¹", "sydney-proxy/claude-opus-4-6-thinking"),
            ("@release", "å‘å¸ƒ Agent", "å‘å¸ƒã€å›æ»šã€å˜æ›´å®¡è®¡", "sydney-proxy/claude-opus-4-6-thinking"),
            ("@research", "è°ƒç ” Agent", "ä¿¡æ¯æ£€ç´¢ã€æ•°æ®åˆ†æã€æŠ¥å‘Šæ²‰æ·€", "sydney-proxy/gpt-5.2-codex"),
        ]
        for code, name, desc, model in default_roles:
            conn.execute(
                """
                INSERT OR IGNORE INTO roles(code, name, description, default_model, enabled, created_at, updated_at)
                VALUES(?,?,?,?,1,?,?)
                """,
                (code, name, desc, model, now_str(), now_str()),
            )

        # é»˜è®¤å…¨å±€å·¥ä½œæµæ¨¡æ¿ï¼ˆå¯å¤ç”¨ï¼Œä¸ç»‘å®šå•ä¸€ä¸šåŠ¡ï¼‰
        default_workflows = [
            (
                "custom_brief",
                "é€šç”¨å£è¯­ä»»åŠ¡",
                "å£è¯­åŒ–æè¿° + é™„ä»¶è¾“å…¥ï¼Œé€‚é…ä»»æ„ä»»åŠ¡",
                json.dumps(["éœ€æ±‚ç†è§£", "æ‰§è¡Œ", "å¤æ ¸", "äº¤ä»˜"], ensure_ascii=False),
                "general",
                "Lead Agent",
                "",
            ),
            (
                "dev_test_verify",
                "å¼€å‘â†’æµ‹è¯•â†’éªŒè¯",
                "é¢å‘ä»£ç ä»»åŠ¡çš„æ ‡å‡†é—­ç¯",
                json.dumps(["å¼€å‘", "æµ‹è¯•", "éªŒè¯", "äº¤ä»˜"], ensure_ascii=False),
                "backend",
                "@developer",
                "",
            ),
            (
                "research_report",
                "è°ƒç ”â†’æç‚¼â†’äº¤ä»˜",
                "é¢å‘ä¿¡æ¯åˆ†æä¸å†…å®¹ç”Ÿäº§ä»»åŠ¡",
                json.dumps(["è°ƒç ”", "æç‚¼", "å¤æ ¸", "äº¤ä»˜"], ensure_ascii=False),
                "research",
                "@research",
                "",
            ),
            (
                "novel_multiagent",
                "å°è¯´ç±»ç›®çˆ†æ¬¾æ–‡åŒ…ï¼ˆå¤šAgentï¼‰",
                "é‡‡é›†â†’æ¸…æ´—â†’å¤æ ¸â†’æ–‡åŒ…",
                json.dumps(["é‡‡é›†", "æ¸…æ´—", "å¤æ ¸", "æ–‡åŒ…"], ensure_ascii=False),
                "research",
                "Lead Agent",
                "",
            ),
            (
                "xhs_virtual_keywords",
                "å°çº¢ä¹¦é«˜é¢‘è¯åˆ†æ",
                "å…³é”®è¯é‡‡é›†ä¸é«˜é¢‘è¯æŠ¥å‘Š",
                json.dumps(["é‡‡é›†", "æ¸…æ´—", "å¤æ ¸", "äº¤ä»˜"], ensure_ascii=False),
                "research",
                "@research",
                "cd __PROJECT_DIR__ && python3 scripts/xhs_virtual_keywords.py --keywords 'è™šæ‹Ÿäº§å“,æ•°å­—äº§å“,PPTæ¨¡æ¿,ç®€å†æ¨¡æ¿,æ•™ç¨‹è¯¾ç¨‹,AIæç¤ºè¯,ç´ æåŒ…,èµ„æ–™åŒ…' --cookie-file $TASK_INPUT_DIR/xhs_cookies.json --scrolls 4 --auto-related 2 --max-keywords 24 --domain general --strict --min-usable 4 --out-md $TASK_OUTPUT_DIR/xhs_virtual_keywords.md --out-json $TASK_OUTPUT_DIR/xhs_virtual_keywords.json",
            ),
        ]
        for code, name, desc, stages, task_type, assignee, cmd in default_workflows:
            conn.execute(
                """
                INSERT OR IGNORE INTO workflows(
                    code, name, description, stages_json, default_task_type, default_assignee, command_template, enabled, created_at, updated_at
                ) VALUES(?,?,?,?,?,?,?,1,?,?)
                """,
                (code, name, desc, stages, task_type, assignee, cmd, now_str(), now_str()),
            )


def get_setting(key: str, default_value: str = "") -> str:
    with db_conn() as conn:
        row = conn.execute("SELECT value FROM app_settings WHERE key=?", (key,)).fetchone()
    if not row:
        return default_value
    return row["value"]


def set_setting(key: str, value: str):
    with db_conn() as conn:
        conn.execute(
            """
            INSERT INTO app_settings(key, value, updated_at)
            VALUES(?,?,?)
            ON CONFLICT(key) DO UPDATE SET value=excluded.value, updated_at=excluded.updated_at
            """,
            (key, str(value), now_str()),
        )


def sync_runtime_settings():
    raw = get_setting("max_concurrent", str(DEFAULT_MAX_CONCURRENT))
    try:
        val = max(1, min(16, int(raw)))
    except Exception:
        val = DEFAULT_MAX_CONCURRENT
    set_setting("max_concurrent", str(val))
    limiter.set_limit(val)


def get_roles(enabled_only: bool = False):
    q = "SELECT * FROM roles"
    if enabled_only:
        q += " WHERE enabled=1"
    q += " ORDER BY CASE WHEN code='Lead Agent' THEN 0 ELSE 1 END, id ASC"
    with db_conn() as conn:
        rows = conn.execute(q).fetchall()
    return rows


def get_workflow_by_code(code: str):
    if not code:
        return None
    with db_conn() as conn:
        return conn.execute("SELECT * FROM workflows WHERE code=?", (code,)).fetchone()


def parse_stages(stages_json: str):
    raw = (stages_json or "").strip()
    if not raw:
        return []
    try:
        data = json.loads(raw)
        if isinstance(data, list):
            return [str(x) for x in data]
    except Exception:
        pass
    return [x.strip() for x in raw.split(",") if x.strip()]


def get_workflows(enabled_only: bool = False):
    q = "SELECT * FROM workflows"
    if enabled_only:
        q += " WHERE enabled=1"
    q += " ORDER BY id ASC"
    with db_conn() as conn:
        rows = conn.execute(q).fetchall()

    out = []
    for r in rows:
        d = dict(r)
        d["stages"] = parse_stages(d.get("stages_json"))
        out.append(d)
    return out


def append_log(task_id: int, line: str):
    with db_conn() as conn:
        conn.execute(
            "INSERT INTO task_logs(task_id, ts, line) VALUES(?,?,?)",
            (task_id, now_str(), line[:4000]),
        )


def update_task(task_id: int, **fields):
    if not fields:
        return
    fields["updated_at"] = now_str()
    keys = list(fields.keys())
    vals = [fields[k] for k in keys]
    clause = ", ".join([f"{k}=?" for k in keys])
    with db_conn() as conn:
        conn.execute(f"UPDATE tasks SET {clause} WHERE id=?", vals + [task_id])


def get_task(task_id: int):
    with db_conn() as conn:
        return conn.execute("SELECT * FROM tasks WHERE id=?", (task_id,)).fetchone()


def login_required(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        if not session.get("logged_in"):
            return redirect(url_for("login"))
        return fn(*args, **kwargs)

    return wrapper


@app.errorhandler(413)
def payload_too_large(_):
    flash("ä¸Šä¼ æ–‡ä»¶è¿‡å¤§ï¼ˆå•æ¬¡è¯·æ±‚ä¸Šé™ 200MBï¼‰ï¼Œè¯·å‹ç¼©åé‡è¯•ã€‚")
    return redirect(url_for("dashboard")), 413


def run_task(task_id: int):
    task = get_task(task_id)
    if not task:
        running_processes.pop(task_id, None)
        return

    with limiter.acquire():
        update_task(task_id, status="running", started_at=now_str(), return_code=None)
        base_dir, input_dir, output_dir = task_artifact_dirs(task_id)
        append_log(task_id, f"[SYSTEM] ä»»åŠ¡å¯åŠ¨ï¼Œå½“å‰å¹¶å‘ä¸Šé™={limiter.get_limit()}")
        append_log(task_id, f"[SYSTEM] ä»»åŠ¡äº§ç‰©ç›®å½•: {base_dir}")
        append_log(task_id, f"[SYSTEM] è¾“å…¥é™„ä»¶ç›®å½•: {input_dir}")
        append_log(task_id, f"[SYSTEM] è¾“å‡ºäº§ç‰©ç›®å½•: {output_dir}")

        cmd = (task["command"] or "").strip()
        if not cmd:
            try:
                for step in [
                    "Lead Agent æ­£åœ¨æ‹†è§£ä»»åŠ¡...",
                    "Backend Agent æ­£åœ¨ç”Ÿæˆæ¥å£å˜æ›´è‰æ¡ˆ...",
                    "Frontend Agent æ­£åœ¨ç”Ÿæˆé¡µé¢æ”¹åŠ¨è‰æ¡ˆ...",
                    "QA Agent æ­£åœ¨å‡†å¤‡å›å½’æµ‹è¯•...",
                    "Lead Agent æ­£åœ¨æ±‡æ€»ç»“æœ...",
                ]:
                    if task_id in running_processes and running_processes[task_id] is None:
                        raise RuntimeError("ä»»åŠ¡è¢«æ‰‹åŠ¨åœæ­¢")
                    append_log(task_id, step)
                    time.sleep(2)
                update_task(task_id, status="done", finished_at=now_str(), return_code=0)
                append_log(task_id, "[SYSTEM] ä»»åŠ¡å®Œæˆï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰")
            except Exception as e:
                update_task(task_id, status="failed", finished_at=now_str(), return_code=1)
                append_log(task_id, f"[SYSTEM] ä»»åŠ¡å¤±è´¥ï¼š{e}")
            finally:
                running_processes.pop(task_id, None)
            return

        try:
            append_log(task_id, f"[SYSTEM] æ‰§è¡Œå‘½ä»¤: {cmd}")
            env = os.environ.copy()
            env.update(
                {
                    "TASK_ID": str(task_id),
                    "TASK_ARTIFACT_DIR": base_dir,
                    "TASK_INPUT_DIR": input_dir,
                    "TASK_OUTPUT_DIR": output_dir,
                }
            )
            proc = subprocess.Popen(
                cmd,
                shell=True,
                cwd=WORKDIR,
                executable="/bin/bash",
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
            )
            running_processes[task_id] = proc

            for line in iter(proc.stdout.readline, ""):
                if not line:
                    break
                append_log(task_id, line.rstrip())

            rc = proc.wait()
            update_task(
                task_id,
                status="done" if rc == 0 else "failed",
                finished_at=now_str(),
                return_code=rc,
            )
            append_log(task_id, f"[SYSTEM] ä»»åŠ¡ç»“æŸï¼Œrc={rc}")
        except Exception as e:
            update_task(task_id, status="failed", finished_at=now_str(), return_code=1)
            append_log(task_id, f"[SYSTEM] æ‰§è¡Œå¼‚å¸¸ï¼š{e}")
        finally:
            running_processes.pop(task_id, None)


def start_task(task_id: int):
    if task_id in running_processes:
        return False, "ä»»åŠ¡å·²åœ¨è¿è¡Œæˆ–æ’é˜Ÿä¸­"
    task = get_task(task_id)
    if not task:
        return False, "ä»»åŠ¡ä¸å­˜åœ¨"
    if task["status"] == "running":
        return False, "ä»»åŠ¡çŠ¶æ€å·²æ˜¯ running"

    running_processes[task_id] = None
    t = threading.Thread(target=run_task, args=(task_id,), daemon=True)
    t.start()
    return True, "å·²å¯åŠ¨ï¼ˆå¦‚å¹¶å‘å·²æ»¡ä¼šè‡ªåŠ¨æ’é˜Ÿï¼‰"


@app.before_request
def _attach_globals():
    g.max_concurrent = limiter.get_limit()
    g.active_workers = limiter.get_running()
    g.artifact_root = ARTIFACT_ROOT
    g.workdir = WORKDIR


@app.route("/healthz")
def healthz():
    return {
        "ok": True,
        "time": now_str(),
        "maxConcurrent": limiter.get_limit(),
        "activeWorkers": limiter.get_running(),
    }


@app.route("/login", methods=["GET", "POST"])
def login():
    if request.method == "POST":
        username = request.form.get("username", "")
        password = request.form.get("password", "")
        if username == ADMIN_USERNAME and password == ADMIN_PASSWORD:
            session["logged_in"] = True
            session["username"] = username
            return redirect(url_for("dashboard"))
        flash("è´¦å·æˆ–å¯†ç é”™è¯¯")
    return render_template("login.html")


@app.route("/logout")
def logout():
    session.clear()
    return redirect(url_for("login"))


@app.route("/")
@login_required
def dashboard():
    with db_conn() as conn:
        tasks = conn.execute(
            "SELECT * FROM tasks ORDER BY CASE status WHEN 'running' THEN 0 WHEN 'pending' THEN 1 ELSE 2 END, id DESC"
        ).fetchall()

        stats = {
            "pending": conn.execute("SELECT COUNT(*) c FROM tasks WHERE status='pending'").fetchone()["c"],
            "running": conn.execute("SELECT COUNT(*) c FROM tasks WHERE status='running'").fetchone()["c"],
            "done": conn.execute("SELECT COUNT(*) c FROM tasks WHERE status='done'").fetchone()["c"],
            "failed": conn.execute("SELECT COUNT(*) c FROM tasks WHERE status='failed'").fetchone()["c"],
        }

    phase_order = ["å¾…ç†è§£", "æ‰§è¡Œä¸­", "å¾…ä½ ç¡®è®¤", "éœ€è¿”å·¥"]
    tasks_by_phase = {k: [] for k in phase_order}
    for t in tasks:
        phase = infer_business_phase(t)
        tasks_by_phase.setdefault(phase, []).append(t)

    roles = get_roles(enabled_only=False)
    workflows = get_workflows(enabled_only=False)

    queue_count = max(0, len(running_processes) - limiter.get_running())
    return render_template(
        "dashboard.html",
        tasks=tasks,
        stats=stats,
        tasks_by_phase=tasks_by_phase,
        phase_order=phase_order,
        roles=roles,
        workflows=workflows,
        running_count=len(running_processes),
        queue_count=queue_count,
    )


@app.route("/artifacts")
@login_required
def artifacts_page():
    files = list_artifacts()
    return render_template("artifacts.html", files=files, artifact_root=ARTIFACT_ROOT)


@app.route("/artifacts/download/<path:rel_path>")
@login_required
def artifacts_download(rel_path: str):
    safe_full = safe_join_under(ARTIFACT_ROOT, rel_path)
    if not safe_full:
        abort(400)
    if not os.path.isfile(safe_full):
        abort(404)
    return send_from_directory(ARTIFACT_ROOT, rel_path, as_attachment=True)


@app.post("/artifacts/clear")
@login_required
def artifacts_clear_note():
    flash("å½“å‰ç‰ˆæœ¬ä¸ºå®‰å…¨èµ·è§æœªå¼€æ”¾ç½‘é¡µåˆ é™¤æ–‡ä»¶ï¼›è¯·åœ¨æœåŠ¡å™¨ä¸Šæ‰‹åŠ¨æ¸…ç†äº§ç‰©ç›®å½•ã€‚")
    return redirect(url_for("artifacts_page"))


@app.post("/settings/concurrency")
@login_required
def set_concurrency():
    raw = (request.form.get("max_concurrent") or "").strip()
    try:
        val = int(raw)
    except Exception:
        flash("å¹¶å‘ä¸Šé™å¿…é¡»æ˜¯æ•°å­—ï¼ˆ1-16ï¼‰")
        return redirect(url_for("dashboard"))

    if val < 1 or val > 16:
        flash("å¹¶å‘ä¸Šé™èŒƒå›´å¿…é¡»åœ¨ 1-16")
        return redirect(url_for("dashboard"))

    set_setting("max_concurrent", str(val))
    limiter.set_limit(val)
    flash(f"å¹¶å‘ä¸Šé™å·²æ›´æ–°ä¸º {val}ï¼ˆå³æ—¶ç”Ÿæ•ˆï¼Œæ— éœ€é‡å¯ï¼‰")
    return redirect(url_for("dashboard"))


@app.post("/roles")
@login_required
def create_role():
    code = (request.form.get("code") or "").strip()
    name = (request.form.get("name") or "").strip()
    description = (request.form.get("description") or "").strip()
    default_model = (request.form.get("default_model") or "").strip()
    enabled = 1 if (request.form.get("enabled") or "1") == "1" else 0

    if not code or not name:
        flash("è§’è‰²åˆ›å»ºå¤±è´¥ï¼šcode å’Œ name ä¸èƒ½ä¸ºç©º")
        return redirect(url_for("dashboard"))

    try:
        with db_conn() as conn:
            conn.execute(
                """
                INSERT INTO roles(code, name, description, default_model, enabled, created_at, updated_at)
                VALUES(?,?,?,?,?,?,?)
                """,
                (code, name, description, default_model, enabled, now_str(), now_str()),
            )
        flash(f"è§’è‰²å·²åˆ›å»ºï¼š{name}ï¼ˆ{code}ï¼‰")
    except sqlite3.IntegrityError:
        flash("è§’è‰²åˆ›å»ºå¤±è´¥ï¼šcode å·²å­˜åœ¨")
    except Exception as e:
        flash(f"è§’è‰²åˆ›å»ºå¤±è´¥ï¼š{e}")

    return redirect(url_for("dashboard"))


@app.post("/roles/<int:role_id>/toggle")
@login_required
def toggle_role(role_id: int):
    with db_conn() as conn:
        row = conn.execute("SELECT enabled, name FROM roles WHERE id=?", (role_id,)).fetchone()
        if not row:
            flash("è§’è‰²ä¸å­˜åœ¨")
            return redirect(url_for("dashboard"))
        nxt = 0 if int(row["enabled"] or 0) == 1 else 1
        conn.execute("UPDATE roles SET enabled=?, updated_at=? WHERE id=?", (nxt, now_str(), role_id))
    flash(f"è§’è‰²å·²{'å¯ç”¨' if nxt == 1 else 'åœç”¨'}ï¼š{row['name']}")
    return redirect(url_for("dashboard"))


@app.post("/workflows")
@login_required
def create_workflow():
    code = (request.form.get("code") or "").strip()
    name = (request.form.get("name") or "").strip()
    description = (request.form.get("description") or "").strip()
    stages_text = (request.form.get("stages") or "").strip()
    default_task_type = (request.form.get("default_task_type") or "general").strip()
    default_assignee = (request.form.get("default_assignee") or "Lead Agent").strip()
    command_template = (request.form.get("command_template") or "").strip()
    enabled = 1 if (request.form.get("enabled") or "1") == "1" else 0

    if not code or not name:
        flash("å·¥ä½œæµåˆ›å»ºå¤±è´¥ï¼šcode å’Œ name ä¸èƒ½ä¸ºç©º")
        return redirect(url_for("dashboard"))

    stages = [x.strip() for x in re.split(r"[,ï¼Œ\n]+", stages_text) if x.strip()]
    stages_json = json.dumps(stages, ensure_ascii=False)

    try:
        with db_conn() as conn:
            conn.execute(
                """
                INSERT INTO workflows(code, name, description, stages_json, default_task_type, default_assignee, command_template, enabled, created_at, updated_at)
                VALUES(?,?,?,?,?,?,?,?,?,?)
                """,
                (code, name, description, stages_json, default_task_type, default_assignee, command_template, enabled, now_str(), now_str()),
            )
        flash(f"å·¥ä½œæµå·²åˆ›å»ºï¼š{name}ï¼ˆ{code}ï¼‰")
    except sqlite3.IntegrityError:
        flash("å·¥ä½œæµåˆ›å»ºå¤±è´¥ï¼šcode å·²å­˜åœ¨")
    except Exception as e:
        flash(f"å·¥ä½œæµåˆ›å»ºå¤±è´¥ï¼š{e}")

    return redirect(url_for("dashboard"))


@app.post("/workflows/<int:workflow_id>/toggle")
@login_required
def toggle_workflow(workflow_id: int):
    with db_conn() as conn:
        row = conn.execute("SELECT enabled, name FROM workflows WHERE id=?", (workflow_id,)).fetchone()
        if not row:
            flash("å·¥ä½œæµä¸å­˜åœ¨")
            return redirect(url_for("dashboard"))
        nxt = 0 if int(row["enabled"] or 0) == 1 else 1
        conn.execute("UPDATE workflows SET enabled=?, updated_at=? WHERE id=?", (nxt, now_str(), workflow_id))
    flash(f"å·¥ä½œæµå·²{'å¯ç”¨' if nxt == 1 else 'åœç”¨'}ï¼š{row['name']}")
    return redirect(url_for("dashboard"))


def derive_title(title_raw: str, brief: str, template_name: str) -> str:
    title = (title_raw or "").strip()
    if title:
        return title
    brief_line = re.sub(r"\s+", " ", (brief or "").strip())
    if brief_line:
        return (brief_line[:36] + "...") if len(brief_line) > 36 else brief_line
    mapping = {
        "novel_multiagent": "å°è¯´ç±»ç›®çˆ†æ¬¾æ–‡åŒ…ä»»åŠ¡",
        "xhs_virtual_keywords": "å°çº¢ä¹¦é«˜é¢‘è¯ä»»åŠ¡",
        "custom_brief": "å£è¯­åŒ–ä»»åŠ¡",
    }
    return mapping.get(template_name, "æ–°ä»»åŠ¡")


def build_command_from_template(template_name: str, project_dir: str, task_brief: str) -> str:
    workdir = (project_dir or WORKDIR).strip() or WORKDIR

    if template_name == "novel_multiagent":
        keywords = "å°è¯´æ¨æ–‡,å°è¯´æ¨è,ç½‘æ–‡,è¨€æƒ…å°è¯´,æ‚¬ç–‘å°è¯´,å®Œç»“å°è¯´,ä¹¦è’æ¨è,ç•ªèŒ„å°è¯´,çˆ½æ–‡å°è¯´,æ¨ç†å°è¯´"
        if "å°çº¢ä¹¦" not in task_brief and "å°è¯´" in task_brief:
            keywords = "å°è¯´æ¨è,ç½‘æ–‡æ¨è,è¨€æƒ…å°è¯´,æ‚¬ç–‘å°è¯´,æ¨ç†å°è¯´,ä¹¦è’æ¨è,å®Œç»“å°è¯´"
        return (
            f"cd {workdir} && python3 scripts/xhs_novel_multiagent_pipeline.py "
            f"--keywords '{keywords}' "
            "--cookie-file $TASK_INPUT_DIR/xhs_cookies.json "
            "--output-dir $TASK_OUTPUT_DIR "
            "--max-rounds 3 --min-usable 8 --min-domain-ratio 0.75 --max-noise-ratio 0.35 "
            "--pack-format zip"
        )

    # ä¼˜å…ˆä½¿ç”¨å·¥ä½œæµä¸­å¿ƒé…ç½®çš„å‘½ä»¤æ¨¡æ¿
    wf = get_workflow_by_code(template_name)
    if wf and (wf["command_template"] or "").strip():
        return (wf["command_template"] or "").replace("__PROJECT_DIR__", workdir)

    if template_name == "xhs_virtual_keywords":
        return (
            f"cd {workdir} && python3 scripts/xhs_virtual_keywords.py "
            "--keywords 'è™šæ‹Ÿäº§å“,æ•°å­—äº§å“,PPTæ¨¡æ¿,ç®€å†æ¨¡æ¿,æ•™ç¨‹è¯¾ç¨‹,AIæç¤ºè¯,ç´ æåŒ…,èµ„æ–™åŒ…' "
            "--cookie-file $TASK_INPUT_DIR/xhs_cookies.json "
            "--scrolls 4 --auto-related 2 --max-keywords 24 --domain general "
            "--strict --min-usable 4 "
            "--out-md $TASK_OUTPUT_DIR/xhs_virtual_keywords.md "
            "--out-json $TASK_OUTPUT_DIR/xhs_virtual_keywords.json"
        )

    return ""


@app.post("/tasks")
@login_required
def create_task():
    workflow_template = (request.form.get("workflow_template") or "custom_brief").strip()
    task_brief = (request.form.get("task_brief") or "").strip()
    delivery_expectation = (request.form.get("delivery_expectation") or "").strip()
    project_dir = (request.form.get("project_dir") or "").strip()

    title = derive_title(request.form.get("title", ""), task_brief, workflow_template)
    if not title:
        flash("è¯·è‡³å°‘å¡«å†™ä»»åŠ¡æè¿°æˆ–æ ‡é¢˜")
        return redirect(url_for("dashboard"))

    description_raw = (request.form.get("description", "") or "").strip()
    desc_parts = []
    if task_brief:
        desc_parts.append(f"ã€ä»»åŠ¡æè¿°ã€‘\n{task_brief}")
    if delivery_expectation:
        desc_parts.append(f"ã€æœŸæœ›äº¤ä»˜ã€‘\n{delivery_expectation}")
    if description_raw:
        desc_parts.append(f"ã€è¡¥å……è¯´æ˜ã€‘\n{description_raw}")
    description = "\n\n".join(desc_parts).strip()

    wf = get_workflow_by_code(workflow_template)

    task_type = (request.form.get("task_type") or "general").strip()
    assignee = (request.form.get("assignee") or "Lead Agent").strip()
    priority = (request.form.get("priority") or "P2").strip()

    # è‹¥æœªæ‰‹å·¥æŒ‡å®šï¼Œä¼˜å…ˆå¥—ç”¨å·¥ä½œæµé»˜è®¤è§’è‰²/ç±»å‹
    if wf:
        if task_type == "general" and (wf["default_task_type"] or "").strip():
            task_type = (wf["default_task_type"] or "general").strip()
        if assignee == "Lead Agent" and (wf["default_assignee"] or "").strip():
            assignee = (wf["default_assignee"] or "Lead Agent").strip()

    command = (request.form.get("command") or "").strip()
    if not command:
        command = build_command_from_template(workflow_template, project_dir, task_brief)

    with db_conn() as conn:
        cur = conn.execute(
            """
            INSERT INTO tasks(title, description, task_type, assignee, priority, status, command, created_at, updated_at)
            VALUES(?,?,?,?,?,'pending',?,?,?)
            """,
            (title, description, task_type, assignee, priority, command, now_str(), now_str()),
        )
        task_id = cur.lastrowid

    append_log(task_id, f"[SYSTEM] ä»»åŠ¡åˆ›å»ºï¼š{title}")
    append_log(task_id, f"[SYSTEM] å·¥ä½œæµæ¨¡æ¿ï¼š{workflow_template}")
    if task_brief:
        append_log(task_id, f"[SYSTEM] å£è¯­åŒ–ä»»åŠ¡æè¿°ï¼š{task_brief[:1200]}")
    if delivery_expectation:
        append_log(task_id, f"[SYSTEM] æœŸæœ›äº¤ä»˜ï¼š{delivery_expectation[:800]}")

    _, input_dir, _ = task_artifact_dirs(task_id)
    uploaded = 0
    for f in request.files.getlist("attachments"):
        if not f or not f.filename:
            continue
        safe_name = secure_filename(f.filename)
        if not safe_name:
            continue
        target = os.path.join(input_dir, safe_name)
        f.save(target)
        uploaded += 1
        append_log(task_id, f"[SYSTEM] å·²ä¸Šä¼ é™„ä»¶: {safe_name}")

    if uploaded > 0:
        flash(f"ä»»åŠ¡ #{task_id} åˆ›å»ºæˆåŠŸï¼Œå·²ä¸Šä¼  {uploaded} ä¸ªé™„ä»¶")
    else:
        flash(f"ä»»åŠ¡ #{task_id} åˆ›å»ºæˆåŠŸ")
    return redirect(url_for("dashboard"))


@app.post("/tasks/<int:task_id>/start")
@login_required
def start_task_route(task_id: int):
    ok, msg = start_task(task_id)
    flash(f"ä»»åŠ¡ #{task_id}: {msg}")
    return redirect(url_for("dashboard"))


@app.post("/tasks/<int:task_id>/retry")
@login_required
def retry_task(task_id: int):
    task = get_task(task_id)
    if not task:
        flash("ä»»åŠ¡ä¸å­˜åœ¨")
        return redirect(url_for("dashboard"))

    update_task(task_id, status="pending", finished_at=None, started_at=None, return_code=None)
    append_log(task_id, "[SYSTEM] ä»»åŠ¡é‡ç½®ä¸º pending")
    flash(f"ä»»åŠ¡ #{task_id} å·²é‡ç½®")
    return redirect(url_for("dashboard"))


@app.post("/tasks/<int:task_id>/stop")
@login_required
def stop_task(task_id: int):
    proc = running_processes.get(task_id)
    if proc is None and task_id in running_processes:
        running_processes.pop(task_id, None)
        update_task(task_id, status="failed", finished_at=now_str(), return_code=137)
        append_log(task_id, "[SYSTEM] ä»»åŠ¡åœ¨å¯åŠ¨é˜¶æ®µè¢«åœæ­¢")
        flash(f"ä»»åŠ¡ #{task_id} å·²åœæ­¢")
        return redirect(url_for("dashboard"))

    if not proc:
        flash("ä»»åŠ¡æœªè¿è¡Œ")
        return redirect(url_for("dashboard"))

    try:
        proc.terminate()
        update_task(task_id, status="failed", finished_at=now_str(), return_code=143)
        append_log(task_id, "[SYSTEM] æ‰‹åŠ¨åœæ­¢ä»»åŠ¡")
        flash(f"ä»»åŠ¡ #{task_id} å·²åœæ­¢")
    except Exception as e:
        flash(f"åœæ­¢å¤±è´¥: {e}")

    return redirect(url_for("dashboard"))


@app.post("/tasks/<int:task_id>/delete")
@login_required
def delete_task(task_id: int):
    task = get_task(task_id)
    if not task:
        flash("ä»»åŠ¡ä¸å­˜åœ¨")
        return redirect(url_for("dashboard"))

    if task_id in running_processes:
        flash(f"ä»»åŠ¡ #{task_id} æ­£åœ¨è¿è¡Œæˆ–æ’é˜Ÿä¸­ï¼Œè¯·å…ˆåœæ­¢åå†åˆ é™¤")
        return redirect(url_for("dashboard"))

    try:
        with db_conn() as conn:
            conn.execute("DELETE FROM task_logs WHERE task_id=?", (task_id,))
            conn.execute("DELETE FROM tasks WHERE id=?", (task_id,))

        task_dir = os.path.join(ARTIFACT_ROOT, f"task_{task_id}")
        if os.path.isdir(task_dir):
            shutil.rmtree(task_dir, ignore_errors=True)

        flash(f"ä»»åŠ¡ #{task_id} å·²åˆ é™¤ï¼ˆå«æ—¥å¿—å’Œä»»åŠ¡é™„ä»¶/äº§ç‰©ï¼‰")
    except Exception as e:
        flash(f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {e}")

    return redirect(url_for("dashboard"))


@app.post("/tasks/<int:task_id>/upload")
@login_required
def task_upload(task_id: int):
    task = get_task(task_id)
    if not task:
        flash("ä»»åŠ¡ä¸å­˜åœ¨")
        return redirect(url_for("dashboard"))

    _, input_dir, _ = task_artifact_dirs(task_id)
    uploaded = 0
    for f in request.files.getlist("attachments"):
        if not f or not f.filename:
            continue
        safe_name = secure_filename(f.filename)
        if not safe_name:
            continue
        target = os.path.join(input_dir, safe_name)
        f.save(target)
        uploaded += 1
        append_log(task_id, f"[SYSTEM] å·²è¿½åŠ ä¸Šä¼ é™„ä»¶: {safe_name}")

    if uploaded == 0:
        flash("æœªæ£€æµ‹åˆ°å¯ä¸Šä¼ æ–‡ä»¶")
    else:
        flash(f"ä»»åŠ¡ #{task_id} é™„ä»¶ä¸Šä¼ å®Œæˆï¼š{uploaded} ä¸ª")
    return redirect(url_for("task_detail", task_id=task_id))


@app.route("/tasks/<int:task_id>/download/<path:rel_path>")
@login_required
def task_artifact_download(task_id: int, rel_path: str):
    task = get_task(task_id)
    if not task:
        abort(404)
    base, _, _ = task_artifact_dirs(task_id)
    safe_full = safe_join_under(base, rel_path)
    if not safe_full:
        abort(400)
    if not os.path.isfile(safe_full):
        abort(404)
    return send_from_directory(base, rel_path, as_attachment=True)


@app.route("/tasks/<int:task_id>")
@login_required
def task_detail(task_id: int):
    task = get_task(task_id)
    if not task:
        return "Task not found", 404

    with db_conn() as conn:
        logs = conn.execute(
            "SELECT ts, line FROM task_logs WHERE task_id=? ORDER BY id ASC", (task_id,)
        ).fetchall()

    base, input_dir, output_dir = task_artifact_dirs(task_id)
    input_files = list_task_files(task_id, "input")
    output_files = list_task_files(task_id, "output")
    delivery = build_delivery_overview(task, output_files, logs)

    return render_template(
        "task_detail.html",
        task=task,
        logs=logs,
        delivery=delivery,
        base_dir=base,
        input_dir=input_dir,
        output_dir=output_dir,
        input_files=input_files,
        output_files=output_files,
    )


@app.route("/api/tasks")
@login_required
def api_tasks():
    with db_conn() as conn:
        rows = conn.execute("SELECT * FROM tasks ORDER BY id DESC LIMIT 200").fetchall()
    return jsonify([dict(r) for r in rows])


if __name__ == "__main__":
    init_db()
    sync_runtime_settings()
    app.run(host="127.0.0.1", port=3100, debug=False)
else:
    init_db()
    sync_runtime_settings()
